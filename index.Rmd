---
title:  'Computational Musicology'
author: 'Max Koste'
date:   'February--March 2020'
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: journal
---

```{r setup}
library(tidyverse)
library(knitr)
library(plotly)
library(spotifyr)
library(compmus)
library(shiny)
library(shinydashboard)
library(ggjoy)
source('spotify.R')

top_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1EtjMPagkNcPdg')
top_2018 <- get_playlist_audio_features('spotify', '37i9dQZF1EjycNLJDHXHis')

top_songs <- top_2019 %>% mutate(playlist = "top 2019") %>%
  bind_rows(top_2018 %>% mutate(playlist = "top 2018"))

theme_max <- function() {
  theme_minimal() +
    theme(
      text = element_text(color = "gray25"),
      plot.subtitle = element_text(size = 12),
      plot.caption = element_text(color = "gray30"),
      plot.background = element_rect(fill = "gray95"),
      plot.margin = unit(c(5, 10, 5, 10), units = "mm")
      
    )
}

```

### tempogram for Mean Mr Mustard shows us what tempo the song is in. 
```{r}
mean_mr_mustard <- get_tidy_audio_analysis('4JOyMhad5dD81uGYLGgKrS')
```

```{r}

mean_mr_mustard %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
  ggtitle("Fourier based tempogram") +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_max()


mean_mr_mustard %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
  ggtitle("Cyclic Tempogram") +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_max()

top_songs %>%
  ggplot(aes(x = tempo, fill = playlist)) + 
  geom_histogram()+ 
  ggtitle("Tempo") + 
  facet_wrap("playlist") + 
  theme_max()

```


*** 
Here we get a clear reading that the bpm is 200 together with a bpm harmonic that says 400. From my knowledge of the song I would say that the bpm is around 100bpm with the markings at 200 and 400 both being bpm harmonics. Wrapping into a cyclic tempogram gets rid of this and shows us that the actual tempo is 100bpm which was suspected. The reason for this clean reading is probably the fact that there are drums present throughout the whole track. The song starts of with a drumfill that you can see as a dipp in the begining of both spectograms. 

### Looking at keys and noticing that Mean Mr Mustard by the beatles doesnt like to get keyprofiled. 
```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3))

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))
```

```{r}
mr_mustard <- 
    get_tidy_audio_analysis('4JOyMhad5dD81uGYLGgKrS') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
    
```

```{r}
mr_mustard %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'B', guide = 'none') +
  ggtitle("Mean Mr Mustard") + 
    theme_max() +
    labs(x = 'Time (s)', y = '')

top_songs%>% 
  ggplot(aes(x = key_name, fill = key_mode)) + 
  geom_histogram(stat = "count") + 
  facet_wrap("playlist") + 
  labs(
    x = "Key Name",
    y = "Count",
    fill = "Key Mode") + 
  theme_max()
```

*** 

Mean Mr Mustard is one of the song with the highest valence in our corpus so lets look at its key structure. 

```{r}

top_songs%>%
  arrange(-valence)%>%
  select(track.name, valence)%>%
  head(5)%>%
  kable()


```

As we can see it is giving use some strange readings since we have both G#m and G in the same section which is very strange. The piece is analysied from bigger segments in order to figure out what key it is in. I will have to further investigate by trying different combinations of normalisations and distances. 

On the histogram we see the number of different keys in our corpus together with if its minor or major indicated by colour. Could not get the colour to be either major/minor, will have to look in to that soo I dont have 1000 colour profiles. 



### What is the SD and Mean Bpm between our playlists? 
```{r}
top_2019_analysis <-
    get_playlist_audio_features(
        'Your  Top Songs 2019', 
        '37i9dQZF1EtjMPagkNcPdg') %>% 
    slice(1:30) %>% 
    add_audio_analysis()
top_2018_analysis <-
    get_playlist_audio_features(
        'Your Top Songs 2018', 
        '37i9dQZF1EjycNLJDHXHis') %>% 
    slice(1:30) %>% 
    add_audio_analysis()
top_songs_analysis <-
    top_2019_analysis %>% mutate(year = "2019") %>%
    bind_rows(top_2018_analysis %>% mutate(year = "2018"))
```

```{r}
top_songs_analysis %>% 
    mutate(
        sections = 
            map(
                sections, 
                summarise_at, 
                vars(tempo, loudness, duration), 
                list(section_mean = mean, section_sd = sd))) %>% 
    unnest(sections) %>%
    ggplot(
        aes(
            x = tempo, 
            y = tempo_section_sd, 
            colour = year, 
            alpha = loudness)) +
    geom_point(aes(size = duration / 60)) + 
    geom_rug() + 
    theme_max() +
    ylim(0, 5) + 
    labs(
        x = 'Mean Tempo (bpm)', 
        y = 'SD Tempo', 
        colour = 'Year', 
        size = 'Duration (min)', 
        alpha = 'Volume (dBFS)')


```


***
Here we look at some of the different metrics in between our playlist with popular albums from the 60s/70s and compare it to our popular modern albums. Our goal here is to look at the standard deviation and our mean when it comes to tempo. For investigation I've added some more dimensions to the data with Duration in min and volume. 

### Looking at Timbre Coefficent between both playlists
```{r}
top_songs_analysis %>% 
    mutate(
        timbre =
            map(
                segments,
                compmus_summarise,
                timbre,
                method = 'mean')) %>%
    select(year, timbre) %>% 
    compmus_gather_timbre %>% 
    ggplot(aes(x = basis, y = value, fill = year)) +
    geom_violin() +
    theme_max ()+
    scale_fill_viridis_d() +
    labs(x = 'Spotify Timbre Coefficients', y = '', fill = 'Year')
```

***

Looking at this we see that there is actually some difference in timbre between the old and the new which comes to no surprise. Around c02 there is a lot of difference in how the value is concentrated, but we also see similairites in the values above c06.

### Tom Mish followed me throughout the years so lets look at the timbre of the song

```{r}
bzt <- 
    get_tidy_audio_analysis('4A7DUET5H4f7dJkUhjfVFB') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))


```

```{r}
bzt %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    scale_fill_viridis_c(option = 'E') +
    theme_max()

bzt %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
  ggtitle("Self Similairity Matric") +
    theme_max() +
    labs(x = '', y = '')

```


***

This song is pretty repeatative so there is not much to say when it comes to the change in timbre. However the changes we do see tells us something about the different segments in the song. This is especially clear in the self similairity matric. Maybe the attractiveness of this song is its clear structure and repeatative nature that made it stick with me throughout 2018 and 2019. Although his guitarplaying also plays a big role. 

### Introduction

I Will look at the differences between the playlist that spotify made for me showcasing the 100 songs I listened to the most in 2018 and 2019. I will look at Bpm, create different types of chromagrams etc to figure out how my music taste varid between the years and what we can see about how spotify clusters songs. 

```{r}
top_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1EtjMPagkNcPdg')
top_2018 <- get_playlist_audio_features('spotify', '37i9dQZF1EjycNLJDHXHis')

top_songs <- top_2019 %>% mutate(playlist = "top 2019") %>%
  bind_rows(top_2018 %>% mutate(playlist = "top 2018"))

```


### Looks like I danced less, atleast listed to less danceable music


```{r}


top_songs_interactive <-
top_songs %>%
  ggplot(aes(x = danceability, y = loudness, col = playlist, label = track.name, size = (valence/energy))) + 
  geom_point(alpha = 0.6, position = "jitter") + 
  geom_rug(size = 0.1)+
  facet_wrap('playlist')+
  theme_max()


ggplotly(top_songs_interactive)
```

***

Here we can clearly see that the songs from 2019 are more spread out and differ more in danceabilty whereas in the 2018 playlist the dots are more clustered towards higher values in danceability.

```{r}
top_songs%>%
  arrange(-valence)%>%
  select(track.name, valence)%>%
  head(5)%>%
  kable()

```





### Chromagram {data-commentary-width=400}

```{r}

wo <- 
    get_tidy_audio_analysis('4A7DUET5H4f7dJkUhjfVFB') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

wo %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_max()
```

***

This is a chromagram of the song lost in paris by Tom Misch




### Conclusion
Fill in when you have all the necessary information and data
