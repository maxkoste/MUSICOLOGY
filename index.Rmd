---
title:  'Computational Musicology'
author: 'Max Koste'
date:   'February--March 2020'
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: journal
---

```{r setup}
library(tidyverse)
library(knitr)
library(plotly)
library(spotifyr)
library(compmus)
library(shiny)
library(tidymodels)
library(protoclust)
library(ggdendro)
library(heatmaply)
library(shinydashboard)
library(ggjoy)
source('spotify.R')

top_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1EtjMPagkNcPdg')
top_2018 <- get_playlist_audio_features('spotify', '37i9dQZF1EjycNLJDHXHis')

top_songs <- top_2019 %>% mutate(playlist = "top 2019") %>%
  bind_rows(top_2018 %>% mutate(playlist = "top 2018"))

theme_max <- function() {
  theme_minimal() +
    theme(
      text = element_text(color = "gray25"),
      plot.subtitle = element_text(size = 12),
      plot.caption = element_text(color = "gray30"),
      plot.background = element_rect(fill = "gray95"),
      plot.margin = unit(c(5, 10, 5, 10), units = "mm")
      
    )
}

```

### Introduction

```{r}
top_2019 <- 
  get_playlist_audio_features('spotify', '37i9dQZF1EtjMPagkNcPdg')
  
top_2018 <-
  get_playlist_audio_features('spotify', '37i9dQZF1EjycNLJDHXHis')

top_songs <- top_2019 %>% mutate(playlist = "top 2019") %>%
  bind_rows(top_2018 %>% mutate(playlist = "top 2018"))

ggplot(top_songs, aes(y = danceability, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Danceability', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of danceability")

ggplot(top_songs, aes(y = liveness, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Liveness', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of liveness")

ggplot(top_songs, aes(y = energy, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Energy', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of energy")

ggplot(top_songs, aes(y = loudness, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Loudness', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of loudness")

ggplot(top_songs, aes(y = speechiness, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Speechiness', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of speechiness")

ggplot(top_songs, aes(y = acousticness, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Acousticness', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of acousticness")

ggplot(top_songs, aes(y = instrumentalness, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Instrumentalness', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of instrumentalness")

ggplot(top_songs, aes(y = valence, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Valence', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of valence")

ggplot(top_songs, aes(y = tempo, x = playlist, fill = playlist)) + 
  geom_violin() + 
  facet_wrap('playlist') + 
  theme_max() +
  labs(
        x = 'Playlist', 
        y = 'Tempo', 
        colour = 'Playlist'
    ) +
  ggtitle("Violin plot of tempo")

ggplot(top_songs, aes(x = key, fill = playlist)) + 
  geom_histogram() + 
  theme_max() +
  facet_wrap("playlist") + 
  labs(
        x = 'Key', 
        y = 'Count', 
        colour = 'Playlist'
    ) +
  ggtitle("histogram of keys")
```


***

I Will look at the differences between the playlist that spotify made for me showcasing the 100 songs I listened to the most in 2018 and 2019. I will look at Bpm, create different types of chromagrams etc to figure out how my music taste varid between the years. First we start with our data. We use use track features from spotifyr to investigate what differences and similairities that we can observe between both playlists. I have combined both playlst into one dataset in order to give a good insight into some of the features from spotifyr and get a quick overview of which of these may be show some clear differences between the playlists.

The track features that might be usefull are the following:

-Danceability

-Energy

-Loudness

-Speechiness

-Accousticness

-Instrumentalness

-Liveness

-Valence

-Tempo

-Key

Noticable differences from the violin plots:

The tempo is lower overall in the newer playlist and valence is higher. There is also alot of quiter tracks in the newer playlist which might indicate that there are more old songs in that playlist that are mastered at a lower volume. 

### Building a model for predicting the most important features. 

```{r}
p2018 <- 
    get_playlist_audio_features('spotify', '37i9dQZF1EtjMPagkNcPdg') %>% 
    slice(1:20) %>% 
    add_audio_analysis
p2019 <- 
    get_playlist_audio_features('spotify', '37i9dQZF1EjycNLJDHXHis') %>% 
    slice(1:20) %>% 
    add_audio_analysis

```

```{r}
top <- 
    p2018 %>% mutate(playlist = "top songs 2018") %>% 
    bind_rows(
        p2019 %>% mutate(playlist = "top songs 2019")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))
```

```{r}
top_class <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = top) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(top) %>% 
    juice
```

```{r}
top_cv <- top_class %>% vfold_cv(5)
```

```{r}
top_knn <- 
    nearest_neighbor(mode = 'classification', neighbors = 1) %>% 
    set_engine('kknn')
predict_knn <- function(split)
    fit(top_knn, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```

```{r echo = FALSE}
top_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class)
```

```{r}
top_cv %>% 
   mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'mosaic')

top_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap')

top_forest <- 
    rand_forest(mode = 'classification') %>% 
    set_engine('randomForest')
predict_forest <- function(split)
    fit(top_forest, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))

top_class %>% 
    fit(top_forest, playlist ~ ., data = .) %>% 
    pluck('fit') %>% 
    randomForest::varImpPlot()+
    theme_max()

```


***

This graphs shows the importance of each predictor when building the model, all predictors are currently visible and can be roughly divided into three groups: - Spotify general features like Acousticness, Valence, Danceability etc, Timbre features and Key features. 
It shows that timbre features c02, c012 and c04 together with acousticness and the key feature G# are the best predictors for our model ranked in importance with mean deacrease gini, ranging from 0.0 t0 1.0. 

### Ploting Liveness and Acousticness from what our model predicted 

```{r}
top %>%
    ggplot(aes(x = acousticness, y = liveness, colour = playlist, size = c02)) +
    geom_point(alpha = 0.8) +
    theme_max()+
    scale_color_brewer(type = 'qual', palette = 'Dark2') +
    labs(
        x = 'Acousticness', 
        y = 'Liveness', 
        size = 'Timbre Feature c02', 
        colour = 'Playlist'
    )
```

```{r eval = FALSE}
top_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```

```{r eval = FALSE}
top_logistic <- 
    logistic_reg(mode = 'classification') %>% 
    set_engine('glm')
predict_logistic <- function(split)
    fit(top_logistic, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```

```{r eval = FALSE}
top_multinom <- 
    multinom_reg(mode = 'classification', penalty = 0.1) %>% 
    set_engine('glmnet')
predict_multinom <- function(split)
    fit(top_multinom, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```

```{r eval = FALSE}
top_tree <- 
    decision_tree(mode = 'classification') %>%
    set_engine('C5.0')
predict_tree <- function(split)
    fit(top_tree, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```


```{r eval = FALSE}
top_class %>% 
    fit(top_tree, playlist ~ ., data = .) %>% 
    pluck('fit') %>%
    summary
```

```{r eval = FALSE}
top_forest <- 
    rand_forest(mode = 'classification') %>% 
    set_engine('randomForest')
predict_forest <- function(split)
    fit(top_forest, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```

```{r eval = FALSE}
top_cv %>% 
    mutate(pred = map(splits, predict_forest)) %>% 
    unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```

```{r eval = FALSE}
predict_knn_reduced <- function(split)
    fit(
        top_knn, 
        playlist ~ c01 + liveness + acousticness + c02 + energy, 
        data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
top_cv %>% 
    mutate(pred = map(splits, predict_knn_reduced)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```

***

The dotplot indicates that we have some outliers in our newer but mainly the 2018 playlist that are quite high in acousticness. The 2019 playlist is mainly clustered towards the lower end of the spectrum when it comes to cousticness and liveness which would indicate a deacrease in more live and instrumental heavy music in the 2018 playlist towards more electronic music in the 2019 playlist. 

### Creating a temporgram for the song mean mr mustard in the corpus and looking at the overall tempo of both playlists

```{r}
mean_mr_mustard <- get_tidy_audio_analysis('4JOyMhad5dD81uGYLGgKrS')
```

```{r}

mean_mr_mustard %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
  ggtitle("Fourier based tempogram") +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_max()


mean_mr_mustard %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
  ggtitle("Cyclic Tempogram") +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_max()

top_songs %>%
  ggplot(aes(x = tempo, fill = playlist)) + 
  geom_histogram()+ 
  ggtitle("Tempogram for both playlist") + 
  facet_wrap("playlist") + 
  theme_max()

```


*** 
Here we get a clear reading that the bpm is 200 together with a bpm harmonic that says 400. From my knowledge of the song I would say that the bpm is around 100bpm with the markings at 200 and 400 both being bpm harmonics. Wrapping into a cyclic tempogram gets rid of this and shows us that the actual tempo is 100bpm which was suspected. The reason for this clean reading is probably the fact that there are drums present throughout the whole track. The song starts of with a drumfill that you can see as a dipp in the begining of both spectograms. 

### Creating a keygram for mean mr mustard. 
```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3))

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))
```

```{r}
mr_mustard <- 
    get_tidy_audio_analysis('4JOyMhad5dD81uGYLGgKrS') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
    
```

```{r}
mr_mustard %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'B', guide = 'none') +
    theme_max() +
    labs(x = 'Time (s)', y = '')

top_songs%>% 
  ggplot(aes(x = key_name, fill = key_mode)) + 
  geom_histogram(stat = "count") + 
  facet_wrap("playlist") + 
  labs(
    x = "Key Name",
    y = "Count",
    fill = "Key Mode") + 
  theme_max()
```

*** 

Mean Mr Mustard is one of the song with the highest valence in our corpus so lets look at its key structure. 

```{r}

top_songs%>%
  arrange(-valence)%>%
  select(track.name, valence)%>%
  head(5)%>%
  kable()


```

As we can see we get a reading that is strong on both G# and G which is because both keys are very close to each other, only being one semitone apart.

On the histogram we see the number of different keys in our corpus together with if its minor or major indicated by colour. 

### Tom Mish followed me throughout the years so lets look at the timbre and key of the song "lost in Paris"

```{r}
bzt <- 
    get_tidy_audio_analysis('4A7DUET5H4f7dJkUhjfVFB') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))


```

```{r}
bzt %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    scale_fill_viridis_c(option = 'E') +
    theme_max()

bzt %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
  ggtitle("Self Similairity Matric") +
    theme_max() +
    labs(x = '', y = '')


wo <- 
    get_tidy_audio_analysis('4A7DUET5H4f7dJkUhjfVFB') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

wo %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_max()
```


***

This song is pretty repeatative so there is not much to say when it comes to the change in timbre. However the changes we do see tells us something about the different segments in the song. This is especially clear in the self similairity matric. Maybe the attractiveness of this song is its clear structure and repeatative nature that made it stick with me throughout 2018 and 2019. Although his guitarplaying also plays a big role. 

The chromagram of the song lost in paris by Tom Misch. The readings are not that clear but we see some activity on C#, F#, B and E which would indicate that this song is in the key of E major. Which is also what I hear when I listen to the song. The chromogram works by 

### Looks like I danced less, atleast listed to less danceable music
```{r}

top_songs %>%
  ggplot(aes(x = acousticness, y = danceability, col = playlist, label = track.name, size = valence)) + 
  geom_point(alpha = 0.6, position = "jitter") + 
  geom_rug(size = 0.1)+
  facet_wrap('playlist') +
  theme_max() + 
  labs( x = "Acousticness",
        y = "Danceability",
        col = "Playlist", 
        size = "Valence")


top_songs %>%
  ggplot(aes(x = danceability, y = valence, col = playlist, label = track.name)) + 
  geom_boxplot(alpha = 0.6) + 
  geom_rug(size = 0.1)+
  facet_wrap('playlist') +
  theme_max() + 
  labs( x = "Danceability",
        y = "Valence",
        col = "Playlist")
```

***

Here we can clearly see that the songs from 2019 are more spread out and differ more in danceabilty whereas in the 2018 playlist the dots are more clustered towards higher values in danceability. Since I moved in a more digital direction with my music taste between 2018 and 2019 I wanted to see what effect it had on the overall danceability. It seems as if it decreased between the two years. The plot is made by using a dataset i created that combines both playlist and then ploting it with danceability and acousticness, which was found to be a good predictor in the model I created. We can also see that the newer playlist is higher in valence which makes it a more happy sounding playlist while it has a lower mean danceability as we can see in the boxplot. 

```{r}
top_songs%>%
  arrange(-valence)%>%
  select(track.name, valence)%>%
  head(5)%>%
  kable()

```

### Conclusion
The trend that have been noticed comes mainly from our prediction model that indicated when further investigated that there is a noticeable difference in acusticness and liveness which indicates a transition from more organically recorded music to a more electronic production. Although the change is not dramatic it is what could be suspected from one year to another. When it comes to timbre there were also differences found in c04, c12 and c02. When it comes to timbre however the findings are hard to interpret since any difference is only indicated by changes in energy. Therefore we cant establish any differenece in instrumentation from timbre alone. However was established by looking at the feautures from spotifyr instead. There is also a difference in valence between both playlist indicating a move towards more happy sounding music. 

The tempo is lower overall in the newer playlist and valence is higher which we could see from the violin plots. There is also alot of quiter tracks in the newer playlist which might indicate that there are more old songs in that playlist that are mastered at a lower volume. However this doesnt necessarily contradict the conclusion that there is a trend towards more electronic music which is often mastered at higher volumes since there only seems to be a few tracks in that playlist mastered at a low volume. 
